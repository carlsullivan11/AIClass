---
title: "AI Project 1"
author: "Carl Sullivan"
date: "2023-02-12"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## kNN Algorithm

The kNN algorithm is a classification algorithm that takes a training set of data and using a distance formula decides the classification of the new data point based on the most common classification of the neighbors. There are several ways to store the training data, either it can all be stored or a certain number of each classification can be stored then storing only the errors. With storing all of the training data the memory costs are much higher than when storing the errors but can result in higher accuracy. 

To implement this algorithm I used Python and stored the data in a Pandas data frame where I was able to take slices of the frame for each training and testing instance. I had considered using dictionaries for the algorithm with the item number being the key and storing an array for the x, y, and class attributes. I went with Pandas though as it gave me an opportunity to practice working with data frames and offered a clear view of the data when debugging. Some troubles I ran into with the data frames though were correct parameters and return types. With data frames, if I took a slice of several rows it would stay as a data frame but when taking a single row using built int Panda functions it can sometimes return a series. 

I ended up with two implementations of the kNN algorithm, the first one 'part1_a' I believe is more correctly written than the second but does not return very accurate results, especially with only storing the errors. The second implementation 'part1_b' returns accurate results but I believe I have a logical error in how I return the distance and vote on the most common classification.

```{r kNN scatter plot}
library(ggplot2)
library(tidyr)
trainData <- read.csv('part1_a/DataCSV/TrainDF0.csv')
testData <- read.csv('part1_a/DataCSV/TestDF0.csv')
store_all <- read.csv('part1_a/DataCSV/StoreAll0.csv')
store_error <- read.csv('part1_a/DataCSV/StoreError0.csv')

trainData %>% 
  ggplot(mapping = aes(x=x,y=y)) +
  geom_text(aes(label= class))

testData %>% 
  ggplot(mapping = aes(x=x,y=y)) +
  geom_text(aes(label= class, color = class))

```

## Simulaged Annealing

The Simulated Annealing algorithm is used to solve optimization problems by a technique of heating and cooling, similar to how metals are hardened. The initial solution is given to the algorithm and from there it is used to find the best solution for a particular problem by iteratively making changes to it. As a new solution is made it is compared to the previous solution and becomes the new solution if better, if it is not better there is a small probability that it could become the new solution. This probability is determined by the temperature function. 

For the Simulated Annealing algorithm, I had difficulty understanding the temperature function and how its role was used in determining the probability and solutions. After I understood how the schedule function determines the temperature by slowly decreasing over time. The probability that the next, even though it may not be a better solution than the current is then determined by the difference between the current and the next. 

I implemented the schedule function with several different values starting with $\frac{t}{1000000}$ and received a value of 7.13182 * $10^{-7}$ for the sphere problem. My initial input vector was [5,5]. Changing the scheduling to $\frac{t}{100}$ with the same problem and same input vector the resulting value is 0.02749. For a 10000% decrease in scheduling rate the optimal solution increased by 38,546%. This shows the drastic role scheduling plays in finding the optimal solution. Without enough time the annealing doesn't have a chance to find minimal solutions to the problem. As seen in the table below the between 100,000 and 1,000,000 the solution starts to level out.

```{r scheduling table, echo=FALSE}
library(knitr)
Scheduling <- read.csv('Scheduling.csv')
kable(Scheduling, caption = 'Scheduling Simulated Annealing on Sphere Problem, input [5,5]', format.args = list(scientific = TRUE))
```

## Genetic Algorithm

The Genetic Algorithm was developed with the idea of natural selection and survival of the fittest, this is due to how it takes two points of data and “breeds” them to make a child with an input of randomness. The solution is found through starting with a population of potential solutions and then iteratively evolving them. 

The Genetic Algorithm was the most difficult for me to implement as there were several helper functions that were necessary for the implementation but weren’t specifically intuitive at first. Fitness, crossover, and mutation are three of the helper functions that are used to consistently change up the data so that it emulates the original set of data. The functions themselves weren’t necessarily difficult the difficult portion was how they fit within the actual algorithm.

In the Genetic Algorithm I kept my population to a small size of 5 of randomly generated individuals of 52 bit float strings. Next was to select two individuals based on the highest fitness score, the fitness was determined by dividing the individual by the summation of the population. Once the fittest parents were selected the values are reproduced by selected a random index and dividing the two parents by this index to cross the strings into two new childeren. The childeren undergo a random mutation if the random number is under a probability. I started with a 5% probability and varied to a 2.5%, 7%, and 10% mutation rate. I also found that 50 generations was enough to return a fit indivual for the solution.


```{r Genetic Algorithm Table, echo=FALSE}
library(knitr)
trials <- read.csv('part2/GAalgoTrials.csv')
kable(trials, caption = 'Genetic Algorithm test tables', format.args = list(scientific = FALSE))

```
