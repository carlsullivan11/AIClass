---
title: "AI Project 1"
author: "Carl Sullivan"
date: "2023-02-12"
output:
  html_document:
    df_print: paged
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## kNN Algorithm

The kNN algorithm is a classification algorithm that takes a training set of data and using a distance formula decides the classification of the new data point based on the most common classification of the neighbors. There are several ways to store the training data, either it can all be stored or a certain number of each classification can be stored then storing only the errors. With storing all of the training data the memory costs are much higher than when storing the errors but can result in higher accuracy. 

To implement this algorithm I used Python and stored the data in a Pandas data frame where I was able to take slices of the frame for each training and testing instance. I had considered using dictionaries for the algorithm with the item number being the key and storing an array for the x, y, and class attributes. I went with Pandas though as it gave me an opportunity to practice working with data frames and offered a clear view of the data when debugging. Some troubles I ran into with the data frames though were correct parameters and return types. With data frames, if I took a slice of several rows it would stay as a data frame but when taking a single row using built int Panda functions it can sometimes return a series. I ended up with two implementations of the kNN algorithm, the first one I believe is correctly written but does not return very accurate results, especially with only storing the errors. The second implementation returns accurate results but I believe I have a logical error in how I return the distance and vote on the most common classification. 

## Simulaged Annealing

The Simulated Annealing algorithm is used to solve optimization problems by a technique of heating and cooling, similar to how metals are hardened. The initial solution is given to the algorithm and from there it is used to find the best solution for a particular problem by iteratively making changes to it. As a new solution is made it is compared to the previous solution and becomes the new solution if better, if it is not better there is a small probability that it could become the new solution. This probability is determined by the temperature function. 

For the Simulated Annealing algorithm, I had difficulty understanding the temperature function and how its role was used in determining the probability and solutions. After I understood how the schedule function determines the temperature by slowly decreasing over time. The probability that the next, even though it may not be a better solution than the current is then determined by the difference between the current and the next. 

## Genetic Algorithm

The Genetic Algorithm was developed with the idea of natural selection and survival of the fittest, this is due to how it takes two points of data and “breeds” them to make a child with an input of randomness. The solution is found through starting with a population of potential solutions and then iteratively evolving them. 

The Genetic Algorithm was the most difficult for me to implement as there were several helper functions that were necessary for the implementation but weren’t specifically intuitive at first. Fitness, crossover, and mutation are three of the helper functions that are used to consistently change up the data so that it emulates the original set of data. The functions themselves weren’t difficult the difficult part was how they fit within the actual algorithm. 


```{r cars}
summary(cars)
```

## Including Plots

You can also embed plots, for example:

```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
